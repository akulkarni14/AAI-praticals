{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/hcTaGuL1ehFk2ZP3b+k5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akulkarni14/AAI-praticals/blob/main/QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlRRoiJ7VqOj",
        "outputId": "f75cf8a2-bfee-4da8-d286-fe1e481f7df0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 complete\n",
            "Episode 100 complete\n",
            "Episode 200 complete\n",
            "Episode 300 complete\n",
            "Episode 400 complete\n",
            "Episode 500 complete\n",
            "Episode 600 complete\n",
            "Episode 700 complete\n",
            "Episode 800 complete\n",
            "Episode 900 complete\n",
            "Q-table after training:\n",
            "[[[-1.57523205 -0.434062   -1.54844577 -0.72974662]\n",
            "  [-2.28617725  0.61694928 -2.21255559 -2.22949177]\n",
            "  [-1.62976704 -0.54729355 -1.7278706  -1.61720273]\n",
            "  [-1.04661746 -1.00889662 -1.23275464 -1.08733323]\n",
            "  [-0.7478178  -0.5572507  -0.73069371 -0.7703044 ]]\n",
            "\n",
            " [[-1.49353246 -0.19404278 -0.66087498  0.62882   ]\n",
            "  [-0.66394879  1.8098     -0.62228005  1.02981189]\n",
            "  [-1.46610917  3.06825685 -1.2419634  -0.17741109]\n",
            "  [-0.72151774  4.33616465 -0.76788621 -0.67407282]\n",
            "  [-0.4833299   5.90608817  0.10846231 -0.13774671]]\n",
            "\n",
            " [[-1.55902976 -1.74632746 -1.36392049  1.72579665]\n",
            "  [ 0.36051051  1.5850504  -0.04629683  3.122     ]\n",
            "  [ 1.19530315  3.6605006   1.71416378  4.58      ]\n",
            "  [ 2.01887695  5.79532474  2.66326948  6.2       ]\n",
            "  [ 3.62172674  8.          4.30157922  4.90288587]]\n",
            "\n",
            " [[-1.21411074 -1.20818862 -1.22478977 -0.84305349]\n",
            "  [-0.74234198 -0.71277918 -0.82258937  4.00540951]\n",
            "  [ 0.84551216  0.77255877 -0.16829612  6.1403364 ]\n",
            "  [ 1.01901743  7.9993147   0.04733563  3.21703096]\n",
            "  [ 5.86663982 10.          5.15938576  7.58048021]]\n",
            "\n",
            " [[-0.95429639 -0.86482753 -0.86482753 -0.85546879]\n",
            "  [-0.61209523 -0.4900995  -0.51895084  0.13850689]\n",
            "  [ 0.23752353 -0.199      -0.21601     5.64304156]\n",
            "  [ 1.93895833  2.39685373  0.97363734  9.99994531]\n",
            "  [ 0.          0.          0.          0.        ]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Grid size (rows x columns)\n",
        "grid_size = (5, 5)\n",
        "\n",
        "# Possible actions: up, down, left, right\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "action_dict = {'up': (-1, 0), 'down': (1, 0), 'left': (0, -1), 'right': (0, 1)}\n",
        "\n",
        "# Q-table initialized to zero\n",
        "Q = np.zeros((grid_size[0], grid_size[1], len(actions)))\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.1  # exploration rate\n",
        "\n",
        "# Define the rewards (for simplicity, just target and penalties)\n",
        "goal = (4, 4)  # target goal position\n",
        "penalty = -1  # penalty for hitting walls or wrong moves\n",
        "reward = 10  # reward for reaching the goal\n",
        "\n",
        "# Function to check if the state is within the grid bounds\n",
        "def is_valid_state(state):\n",
        "    return 0 <= state[0] < grid_size[0] and 0 <= state[1] < grid_size[1]\n",
        "\n",
        "# Function to select an action using epsilon-greedy strategy\n",
        "def select_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        # Exploration: choose a random action\n",
        "        return random.choice(actions)\n",
        "    else:\n",
        "        # Exploitation: choose the action with the highest Q-value\n",
        "        action_index = np.argmax(Q[state[0], state[1]])\n",
        "        return actions[action_index]\n",
        "\n",
        "# Function to move the agent in the grid\n",
        "def move(state, action):\n",
        "    move_delta = action_dict[action]\n",
        "    new_state = (state[0] + move_delta[0], state[1] + move_delta[1])\n",
        "    if is_valid_state(new_state):\n",
        "        return new_state\n",
        "    else:\n",
        "        # If the move is out of bounds, stay in the same position\n",
        "        return state\n",
        "\n",
        "# Function to train the agent using Q-learning\n",
        "def train_agent(episodes=1000):\n",
        "    for episode in range(episodes):\n",
        "        state = (0, 0)  # Start at the top-left corner\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Select an action\n",
        "            action = select_action(state)\n",
        "\n",
        "            # Perform the action and get the next state\n",
        "            next_state = move(state, action)\n",
        "\n",
        "            # Get the reward for the new state\n",
        "            if next_state == goal:\n",
        "                reward_received = reward\n",
        "                done = True  # Goal reached\n",
        "            else:\n",
        "                reward_received = penalty\n",
        "\n",
        "            # Update the Q-value for the state-action pair\n",
        "            action_index = actions.index(action)\n",
        "            next_max = np.max(Q[next_state[0], next_state[1]])  # max Q-value for the next state\n",
        "            Q[state[0], state[1], action_index] = Q[state[0], state[1], action_index] + alpha * (reward_received + gamma * next_max - Q[state[0], state[1], action_index])\n",
        "\n",
        "            # Update the state\n",
        "            state = next_state\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode} complete\")\n",
        "\n",
        "# Run the Q-learning algorithm\n",
        "train_agent(episodes=1000)\n",
        "\n",
        "# After training, let's print the Q-table\n",
        "print(\"Q-table after training:\")\n",
        "print(Q)\n"
      ]
    }
  ]
}